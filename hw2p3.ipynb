{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2 - Part 3: Q5 (RNN and GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darkghost/anaconda3/envs/ml/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/darkghost/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Importing all libraries\n",
    "'''\n",
    "from copy import deepcopy\n",
    "from numpy import argmax\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "import gensim\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import vstack\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import gensim.downloader as api\n",
    "from sklearn.svm import LinearSVC as SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.linear_model import Perceptron\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('wordnet')\n",
    "warnings.filterwarnings('ignore')\n",
    "CUDA_LAUNCH_BLOCKING = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTranformation(object):\n",
    "\n",
    "    def __init__(self, filename, preprocess):\n",
    "        self.filename = filename\n",
    "        self.random_state = 10\n",
    "        self.n = 50000\n",
    "        self.preprocess = preprocess\n",
    "        print(\"Preproces: \" + str(preprocess))\n",
    "\n",
    "    def read_file(self, error_bad_lines=False, warn_bad_lines=False, sep=\"\\t\"):\n",
    "        df = pd.read_csv(self.filename, sep=sep,\n",
    "                         error_bad_lines=error_bad_lines, warn_bad_lines=warn_bad_lines)\n",
    "        df = df.dropna()\n",
    "        return df\n",
    "\n",
    "    def formation(self, row1='review_body', row2='star_rating', ):\n",
    "        df = self.read_file()\n",
    "        df = df[[row1, row2]]\n",
    "        df = df.dropna()\n",
    "\n",
    "        dataset = pd.concat([df[df['star_rating'] == 1].sample(n=50000, random_state=10),\n",
    "                             df[df['star_rating'] == 2].sample(\n",
    "                                 n=50000, random_state=10),\n",
    "                             df[df['star_rating'] == 3].sample(\n",
    "                                 n=50000, random_state=10),\n",
    "                             df[df['star_rating'] == 4].sample(\n",
    "                                 n=50000, random_state=10),\n",
    "                             df[df['star_rating'] == 5].sample(n=50000, random_state=10)])\n",
    "\n",
    "        dataset = dataset.reset_index(drop=True)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def label(self, rows):\n",
    "        if rows.star_rating > 3:\n",
    "            return 1\n",
    "        elif rows.star_rating < 3:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    def apply_label(self):\n",
    "        dataset = self.formation()\n",
    "        dataset['label'] = dataset.apply(lambda row: self.label(row), axis=1)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def remove_html_and_url(self, s):\n",
    "        s = re.sub(\n",
    "            r'(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)', '', s, flags=re.MULTILINE)\n",
    "        soup = BeautifulSoup(s, 'html.parser')\n",
    "        s = soup.get_text()\n",
    "        return s\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        text_tokens = word_tokenize(s)\n",
    "        return text_tokens\n",
    "\n",
    "    def without_preprocess(self):\n",
    "        dataset = self.apply_label()\n",
    "        dataset.review_body = dataset.review_body.apply(self.tokenize)\n",
    "        return dataset\n",
    "\n",
    "    def with_preprocess(self):\n",
    "        dataset = self.apply_label()\n",
    "        dataset.review_body = dataset.review_body.str.lower()\n",
    "\n",
    "        dataset.review_body = dataset.review_body.apply(\n",
    "            lambda s: self.remove_html_and_url(s))\n",
    "        dataset.review_body = dataset.review_body.apply(\n",
    "            lambda s: re.sub(\"[^a-zA-Z']+\", \" \", s))\n",
    "        dataset.review_body = dataset.review_body.apply(\n",
    "            lambda s: re.sub(' +', ' ', s))\n",
    "\n",
    "        dataset.review_body = dataset.review_body.apply(self.tokenize)\n",
    "\n",
    "        dataset.dropna()\n",
    "        return dataset\n",
    "\n",
    "    def train_test_split(self):\n",
    "\n",
    "        if self.preprocess:\n",
    "            dataset = self.with_preprocess()\n",
    "        else:\n",
    "            dataset = self.without_preprocess()\n",
    "\n",
    "        train = dataset.sample(frac=0.8, random_state=200)\n",
    "        test = dataset.drop(train.index)\n",
    "        train = train.reset_index(drop=True)\n",
    "        test = test.reset_index(drop=True)\n",
    "\n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorization(object):\n",
    "\n",
    "    def __init__(self, model, dataset, model_type=\"model\", classification=\"binary\", mode=\"mean\", pad=False):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.model_type = model_type \n",
    "        self.classification = classification  \n",
    "        if self.model_type == \"pretrained\":\n",
    "            self.vocab = self.model\n",
    "        if self.model_type == \"model\":\n",
    "            self.vocab = self.model.wv\n",
    "\n",
    "        self.mode = mode\n",
    "        self.pad = pad\n",
    "\n",
    "    def get_mean_vector(self, data_review_body, data_label):\n",
    "\n",
    "        if self.classification == \"binary\":\n",
    "            if data_label != 3:\n",
    "                if self.model_type == \"model\":\n",
    "                    words = [\n",
    "                        word for word in data_review_body if word in self.vocab.index_to_key]\n",
    "                    if len(words) >= 1:\n",
    "                        rev = []\n",
    "                        for word in words:\n",
    "                            rev.append(np.array(self.vocab[word]))\n",
    "\n",
    "                        if type(data_label) is not int:\n",
    "                            print(\"Found\")\n",
    "                        return rev, data_label\n",
    "                else:\n",
    "                    words = [\n",
    "                        word for word in data_review_body if word in self.vocab]\n",
    "                    if len(words) >= 1:\n",
    "                        rev = []\n",
    "                        for word in words:\n",
    "                            rev.append(np.array(self.vocab[word]))\n",
    "\n",
    "                        if type(data_label) is not int:\n",
    "                            print(\"Found\")\n",
    "                        return rev, data_label\n",
    "\n",
    "        else:\n",
    "            if self.model_type == \"mode\":\n",
    "                words = [\n",
    "                    word for word in data_review_body if word in self.vocab.index_to_key]\n",
    "                if len(words) >= 1:\n",
    "                    rev = []\n",
    "                    for word in words:\n",
    "                        rev.append(np.array(self.vocab[word]))\n",
    "                    return rev, data_label\n",
    "            else:\n",
    "                words = [word for word in data_review_body if word in self.vocab]\n",
    "                if len(words) >= 1:\n",
    "                    rev = []\n",
    "                    for word in words:\n",
    "                        rev.append(np.array(self.vocab[word]))\n",
    "                    return rev, data_label\n",
    "\n",
    "    def feature_extraction(self):\n",
    "        feature = []\n",
    "        y_label = []\n",
    "        for data_review_body, data_label in zip(self.dataset.review_body, self.dataset.label):\n",
    "            try:\n",
    "                x, y = self.get_mean_vector(data_review_body, data_label)\n",
    "                if self.pad:\n",
    "                    if len(x) >= 50:\n",
    "                        feature.append(x[:50])\n",
    "                        y_label.append(y)\n",
    "                    else:\n",
    "                        feature.append(x)\n",
    "                        y_label.append(y)\n",
    "                else:\n",
    "                    if self.mode == \"vec\":\n",
    "                        if len(x) >= 10:\n",
    "                            feature.append(x[:10])\n",
    "                            y_label.append(y)\n",
    "                    else:\n",
    "                        feature.append(np.mean(x, axis=0))\n",
    "                        y_label.append(y)\n",
    "            except:\n",
    "                pass\n",
    "        print(\"Vectorization Completed\")\n",
    "        return feature, y_label\n",
    "\n",
    "    def pad_review(self, review, seq_len):\n",
    "\n",
    "        features = np.zeros((seq_len, 300), dtype=float)\n",
    "        features[-len(review):] = np.array(review)[:seq_len]\n",
    "\n",
    "        return features\n",
    "\n",
    "    def join_words(self, x):\n",
    "        y = \"\"\n",
    "        for ele in x:\n",
    "            y = ' '.join(ele)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence(object):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __iter__(self):\n",
    "        for row in self.dataset:\n",
    "            yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Data(Dataset):\n",
    "\n",
    "    def __init__(self, X_data, Y_data):\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pad = np.zeros((50, 300), dtype=float)\n",
    "        pad[-len(self.X_data[index]):] = np.array(self.X_data[index])[:50]\n",
    "        X = torch.FloatTensor(pad)\n",
    "        Y = torch.tensor(self.Y_data[index])\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, model_type=\"rnn\"):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.model_type = model_type\n",
    "\n",
    "        if self.model_type == \"gru\":\n",
    "            self.layer = nn.GRU(input_size, hidden_dim,\n",
    "                                n_layers, batch_first=True)\n",
    "        else:\n",
    "            self.layer = nn.RNN(input_size, hidden_dim,\n",
    "                                n_layers, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(2500, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.layer(x, hidden)\n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, out.shape[1] * out.shape[2])\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preproces: True\n"
     ]
    }
   ],
   "source": [
    "filename = \"./amazon_reviews_us_Kitchen_v1_00.tsv\"\n",
    "dt = DataTranformation(filename, True)\n",
    "train, test = dt.train_test_split()\n",
    "sentences = Sentence(train['review_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = api.load('word2vec-google-news-300')\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences, vector_size=300, min_count=10, window=11, seed=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    '''\n",
    "     collate_fn is your callable/function that processes the batch you want to return from your dataloader\n",
    "    '''\n",
    "    data = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def rnn_train(model, epoch, dataset_x, dataset_y, name):\n",
    "    \n",
    "    rnn_train = RNN_Data(dataset_x, dataset_y)\n",
    "    train_loader_mode = DataLoader(dataset = rnn_train, batch_size=8, shuffle = True, collate_fn=my_collate, drop_last=True)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    for ep in range(1, epoch + 1):\n",
    "        \n",
    "        for input_data, label in train_loader_mode:\n",
    "            optimizer.zero_grad()\n",
    "            input_data = torch.stack(input_data)\n",
    "            label = torch.stack(label)\n",
    "            output, hidden = model(input_data.to(device))\n",
    "            loss = criterion(output, label.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # print('Epoch: {} \\tTraining Loss: {:.6f}'.format(ep, loss.item()))\n",
    "        torch.save(model.state_dict(), name + str(ep) + '.pt')\n",
    "        \n",
    "def rnn_test(model, epoch, dataset_x, dataset_y, name):\n",
    "    \n",
    "    rnn_test = RNN_Data(dataset_x, dataset_y)\n",
    "    test_loader_mode = DataLoader(dataset = rnn_test, batch_size=8, collate_fn=my_collate, drop_last=True)\n",
    "    tmp = 0\n",
    "    for i in range(1, epoch+1):\n",
    "        model.load_state_dict(torch.load(name +str(i) + '.pt'))\n",
    "        model = model.to(device)\n",
    "        \n",
    "        predictions, actual = list(), list()\n",
    "        for test_data, test_label in test_loader_mode:\n",
    "            test_data = torch.stack(test_data)\n",
    "            test_label = torch.stack(test_label)\n",
    "            pred, hid = model(test_data.to('cuda'))\n",
    "            pred = pred.to('cpu')\n",
    "            pred = pred.detach().numpy()\n",
    "            pred = argmax(pred, axis= 1)\n",
    "            target = test_label.numpy()\n",
    "            target = target.reshape((len(target), 1))\n",
    "            pred = pred.reshape((len(pred)), 1)\n",
    "            pred = pred.round()\n",
    "            predictions.append(pred)\n",
    "            actual.append(target)\n",
    "                \n",
    "        predictions, actual = vstack(predictions), vstack(actual)\n",
    "        acc = accuracy_score(actual, predictions)\n",
    "        print('Accuracy: %.3f' % acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN and GRU with binary and Self Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization Completed\n",
      "Accuracy: 0.757\n",
      "Accuracy: 0.781\n"
     ]
    }
   ],
   "source": [
    "rnn_bin = Model(300, 3, 50, 1)\n",
    "rnn_bin = rnn_bin.to(device)\n",
    "gru_model_bin = Model(300, 3, 50, 1, model_type=\"gru\")\n",
    "gru_model_bin = gru_model_bin.to(device)\n",
    "\n",
    "vec_rnn_train = Vectorization(model, train, classification = \"binary\", pad = True)\n",
    "vec_rnn_test = Vectorization(model, test, classification =\"binary\", pad = True)\n",
    "\n",
    "X_rnn_train, Y_rnn_train = vec_rnn_train.feature_extraction()\n",
    "X_rnn_test, Y_rnn_test = vec_rnn_test.feature_extraction()\n",
    "\n",
    "rnn_train(rnn_bin, 10, X_rnn_train, Y_rnn_train, name = \"rnn_model\")\n",
    "rnn_test(rnn_bin, 10, X_rnn_test, Y_rnn_test, name = \"rnn_model\")\n",
    "\n",
    "rnn_train(gru_model_bin, 10, X_rnn_train, Y_rnn_train, name = \"gru_model\")\n",
    "rnn_test(gru_model_bin, 10, X_rnn_test, Y_rnn_test, name = \"gru_model\")\n",
    "\n",
    "del vec_rnn_train, vec_rnn_test, X_rnn_train, X_rnn_test, Y_rnn_train, Y_rnn_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN and GRU with multi-classification self trained w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization Completed\n",
      "RNN: \n",
      "Accuracy: 0.581\n",
      "GRU: \n",
      "Accuracy: 0.601\n"
     ]
    }
   ],
   "source": [
    "rnn = Model(300, 4, 50, 1)\n",
    "rnn = rnn.to(device)\n",
    "vec_rnn_multi_train = Vectorization(model, train, classification = \"multi-class\", pad = True)\n",
    "vec_rnn_multi_test = Vectorization(model, test, classification = \"multi-class\", pad = True)\n",
    "\n",
    "X_rnn_multi_train, Y_rnn_multi_train = vec_rnn_multi_train.feature_extraction()\n",
    "X_rnn_multi_test, Y_rnn_multi_test = vec_rnn_multi_test.feature_extraction()\n",
    "print(\"RNN: \")\n",
    "rnn_train(rnn, 10, X_rnn_multi_train, Y_rnn_multi_train, name = \"rnn_multi_model\")\n",
    "rnn_test(rnn, 10, X_rnn_multi_test, Y_rnn_multi_test, name = \"rnn_multi_model\")\n",
    "\n",
    "gru_model = Model(300, 4, 50, 1, model_type=\"gru\")\n",
    "gru_model = gru_model.to(device)\n",
    "print(\"GRU: \")\n",
    "rnn_train(gru_model, 10, X_rnn_multi_train, Y_rnn_multi_train, name = \"gru_multi_model\")\n",
    "rnn_test(gru_model, 10, X_rnn_multi_test, Y_rnn_multi_test, name = \"gru_multi_model\")\n",
    "\n",
    "del vec_rnn_multi_train, vec_rnn_multi_test, Y_rnn_multi_train, X_rnn_multi_test, Y_rnn_multi_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN and GRU with binary and pre-trained w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization Completed\n",
      "RNN:\n",
      "Accuracy: 0.822\n",
      "GRU: \n",
      "Accuracy: 0.871\n"
     ]
    }
   ],
   "source": [
    "vec_rnn_pre_train = Vectorization(model = pretrained_model, dataset = train, model_type=\"pretrained\", classification = \"binary\", mode = \"vec\", pad = True)\n",
    "vec_rnn_pre_test = Vectorization(model = pretrained_model, dataset = test, model_type = \"pretrained\", classification = \"binary\", mode = \"vec\", pad = True)\n",
    "\n",
    "X_rnn_pre_train, Y_rnn_pre_train = vec_rnn_pre_train.feature_extraction()\n",
    "X_rnn_pre_test, Y_rnn_pre_test = vec_rnn_pre_test.feature_extraction()\n",
    "\n",
    "print(\"RNN:\")\n",
    "rnn_train(rnn_bin, 10, X_rnn_pre_train, Y_rnn_pre_train, name = \"rnn_pre_model\")\n",
    "rnn_test(rnn_bin, 10, X_rnn_pre_test, Y_rnn_pre_test, name = \"rnn_pre_model\")\n",
    "\n",
    "print(\"GRU: \")\n",
    "rnn_train(gru_model_bin, 10, X_rnn_pre_train, Y_rnn_pre_train, name = \"gru_pre_model\")\n",
    "rnn_test(gru_model_bin, 10, X_rnn_pre_test, Y_rnn_pre_test, name = \"gru_pre_model\")\n",
    "\n",
    "del vec_rnn_pre_train, vec_rnn_pre_test,  X_rnn_pre_train, Y_rnn_pre_train, X_rnn_pre_test, Y_rnn_pre_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN and GRU with multi-class and Pretrained w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization Completed\n",
      "RNN:\n",
      "Accuracy: 0.702\n",
      "GRU: \n",
      "Accuracy: 0.738\n"
     ]
    }
   ],
   "source": [
    "vec_rnn_pre_multi_train = Vectorization(model = pretrained_model, dataset = train, model_type = \"pretrained\", classification = \"multi-class\", mode = \"vec\", pad = True)\n",
    "vec_rnn_pre_multi_test = Vectorization(model = pretrained_model, dataset = test, model_type = \"pretrained\", classification = \"multi-class\", mode = \"vec\", pad = True)\n",
    "\n",
    "X_rnn_pre_multi_train, Y_rnn_pre_multi_train = vec_rnn_pre_multi_train.feature_extraction()\n",
    "X_rnn_pre_multi_test, Y_rnn_pre_multi_test = vec_rnn_pre_multi_test.feature_extraction()\n",
    "print(\"RNN:\")\n",
    "rnn_train(rnn, 10, X_rnn_pre_multi_train, Y_rnn_pre_multi_train, name = \"rnn_pre_model_multi\")\n",
    "rnn_test(rnn, 10, X_rnn_pre_multi_test, Y_rnn_pre_multi_test, name = \"rnn_pre_model_multi\")\n",
    "print(\"GRU: \")\n",
    "rnn_train(gru_model, 10, X_rnn_pre_multi_train, Y_rnn_pre_multi_train, name = \"gru_pre_model_multi\")\n",
    "rnn_test(gru_model, 10, X_rnn_pre_multi_test, Y_rnn_pre_multi_test, name = \"gru_pre_model_multi\")\n",
    "\n",
    "del vec_rnn_pre_multi_train, vec_rnn_pre_multi_test,  X_rnn_pre_multi_train, Y_rnn_pre_multi_train, X_rnn_pre_multi_test, Y_rnn_pre_multi_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "\n",
    "GRU gives better accuracy compare to RNN in all cases with this data.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9cb23f9fb748a30e8d8bc81b9adb518e023d0772aa87870f409f813d831654e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
