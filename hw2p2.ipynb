{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## HW-2 Part-2 - consists of Q4 of hw2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "'''\n",
    "Importing all libraries\n",
    "'''\n",
    "from copy import deepcopy\n",
    "from numpy import argmax\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "import gensim\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import vstack\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import gensim.downloader as api\n",
    "from sklearn.svm import LinearSVC as SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.linear_model import Perceptron\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('wordnet')\n",
    "warnings.filterwarnings('ignore')\n",
    "CUDA_LAUNCH_BLOCKING = 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/darkghost/anaconda3/envs/ml/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/darkghost/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformation class for pre-processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class DataTranformation(object):\n",
    "\n",
    "    def __init__(self, filename, preprocess):\n",
    "        self.filename = filename\n",
    "        self.random_state = 10\n",
    "        self.n = 50000\n",
    "        self.preprocess = preprocess\n",
    "        print(\"Preproces: \" + str(preprocess))\n",
    "\n",
    "    def read_file(self, error_bad_lines=False, warn_bad_lines=False, sep=\"\\t\"):\n",
    "        df = pd.read_csv(self.filename, sep=sep,\n",
    "                         error_bad_lines=error_bad_lines, warn_bad_lines=warn_bad_lines)\n",
    "        df = df.dropna()\n",
    "        return df\n",
    "\n",
    "    def df_formation(self, row1='review_body', row2='star_rating', ):\n",
    "        df = self.read_file()\n",
    "        df = df[[row1, row2]]\n",
    "        df = df.dropna()\n",
    "\n",
    "        dataset = pd.concat([df[df['star_rating'] == 1].sample(n=50000, random_state=10),\n",
    "                             df[df['star_rating'] == 2].sample(\n",
    "                                 n=50000, random_state=10),\n",
    "                             df[df['star_rating'] == 3].sample(\n",
    "                                 n=50000, random_state=10),\n",
    "                             df[df['star_rating'] == 4].sample(\n",
    "                                 n=50000, random_state=10),\n",
    "                             df[df['star_rating'] == 5].sample(n=50000, random_state=10)])\n",
    "\n",
    "        dataset = dataset.reset_index(drop=True)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def label(self, rows):\n",
    "        if rows.star_rating > 3:\n",
    "            return 1\n",
    "        elif rows.star_rating < 3:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    def apply_label(self):\n",
    "        dataset = self.df_formation()\n",
    "        dataset['label'] = dataset.apply(lambda row: self.label(row), axis=1)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def remove_html_and_url(self, s):\n",
    "        s = re.sub(\n",
    "            r'(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)', '', s, flags=re.MULTILINE)\n",
    "        soup = BeautifulSoup(s, 'html.parser')\n",
    "        s = soup.get_text()\n",
    "        return s\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        text_tokens = word_tokenize(s)\n",
    "        return text_tokens\n",
    "\n",
    "    def without_preprocess(self):\n",
    "        dataset = self.apply_label()\n",
    "        dataset.review_body = dataset.review_body.apply(self.tokenize)\n",
    "        return dataset\n",
    "\n",
    "    def with_preprocess(self):\n",
    "        dataset = self.apply_label()\n",
    "        dataset.review_body = dataset.review_body.str.lower()\n",
    "\n",
    "        dataset.review_body = dataset.review_body.apply(\n",
    "            lambda s: self.remove_html_and_url(s))\n",
    "        dataset.review_body = dataset.review_body.apply(\n",
    "            lambda s: re.sub(\"[^a-zA-Z']+\", \" \", s))\n",
    "        dataset.review_body = dataset.review_body.apply(\n",
    "            lambda s: re.sub(' +', ' ', s))\n",
    "\n",
    "        dataset.review_body = dataset.review_body.apply(self.tokenize)\n",
    "\n",
    "        dataset.dropna()\n",
    "        return dataset\n",
    "\n",
    "    def train_test_split(self):\n",
    "\n",
    "        if self.preprocess:\n",
    "            dataset = self.with_preprocess()\n",
    "        else:\n",
    "            dataset = self.without_preprocess()\n",
    "\n",
    "        train = dataset.sample(frac=0.8, random_state=200)\n",
    "        test = dataset.drop(train.index)\n",
    "        train = train.reset_index(drop=True)\n",
    "        test = test.reset_index(drop=True)\n",
    "\n",
    "        return train, test\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vectorization Class for feature Extraction "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class Vectorization(object):\n",
    "\n",
    "    def __init__(self, model, dataset, model_type=\"model\", classification=\"binary\", mode=\"mean\", pad=False):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.model_type = model_type  \n",
    "        self.classification = classification  \n",
    "        if self.model_type == \"pretrained\":\n",
    "            self.vocab = self.model\n",
    "        if self.model_type == \"model\":\n",
    "            self.vocab = self.model.wv\n",
    "\n",
    "        self.mode = mode\n",
    "        self.pad = pad\n",
    "\n",
    "    def get_mean_vector(self, data_review_body, data_label):\n",
    "\n",
    "        if self.classification == \"binary\":\n",
    "            if data_label != 3:\n",
    "                if self.model_type == \"model\":\n",
    "                    words = [\n",
    "                        word for word in data_review_body if word in self.vocab.index_to_key]\n",
    "                    if len(words) >= 1:\n",
    "                        rev = []\n",
    "                        for word in words:\n",
    "                            rev.append(np.array(self.vocab[word]))\n",
    "\n",
    "                        if type(data_label) is not int:\n",
    "                            print(\"Found\")\n",
    "                        return rev, data_label\n",
    "                else:\n",
    "                    words = [\n",
    "                        word for word in data_review_body if word in self.vocab]\n",
    "                    if len(words) >= 1:\n",
    "                        rev = []\n",
    "                        for word in words:\n",
    "                            rev.append(np.array(self.vocab[word]))\n",
    "\n",
    "                        if type(data_label) is not int:\n",
    "                            print(\"Found\")\n",
    "                        return rev, data_label\n",
    "\n",
    "        else:\n",
    "            if self.model_type == \"mode\":\n",
    "                words = [\n",
    "                    word for word in data_review_body if word in self.vocab.index_to_key]\n",
    "                if len(words) >= 1:\n",
    "                    rev = []\n",
    "                    for word in words:\n",
    "                        rev.append(np.array(self.vocab[word]))\n",
    "                    return rev, data_label\n",
    "            else:\n",
    "                words = [word for word in data_review_body if word in self.vocab]\n",
    "                if len(words) >= 1:\n",
    "                    rev = []\n",
    "                    for word in words:\n",
    "                        rev.append(np.array(self.vocab[word]))\n",
    "                    return rev, data_label\n",
    "\n",
    "    def feature_extraction(self):\n",
    "        feature = []\n",
    "        y_label = []\n",
    "        # print(self.vocab.index_to_key)\n",
    "        for data_review_body, data_label in zip(self.dataset.review_body, self.dataset.label):\n",
    "            try:\n",
    "                x, y = self.get_mean_vector(data_review_body, data_label)\n",
    "                if self.pad:\n",
    "                    if len(x) >= 50:\n",
    "                        feature.append(x[:50])\n",
    "                        y_label.append(y)\n",
    "                    else:\n",
    "                        feature.append(x)\n",
    "                        y_label.append(y)\n",
    "                else:\n",
    "                    if self.mode == \"vec\":\n",
    "                        if len(x) >= 10:\n",
    "                            feature.append(x[:10])\n",
    "                            y_label.append(y)\n",
    "                    else:\n",
    "                        feature.append(np.mean(x, axis=0))\n",
    "                        y_label.append(y)\n",
    "            except:\n",
    "                pass\n",
    "        print(\"Vectorization Completed\")\n",
    "        return feature, y_label\n",
    "\n",
    "    def pad_review(self, review, seq_len):\n",
    "\n",
    "        features = np.zeros((seq_len, 300), dtype=float)\n",
    "        features[-len(review):] = np.array(review)[:seq_len]\n",
    "\n",
    "        return features\n",
    "\n",
    "    def join_words(self, x):\n",
    "        y = \"\"\n",
    "        for ele in x:\n",
    "            y = ' '.join(ele)\n",
    "        return y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class Sentence(object):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __iter__(self):\n",
    "        for row in self.dataset:\n",
    "            yield row"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-layer Perceptron using average word2Vec similar to \"Simple Models\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, classification=\"binary\", vocab_size=300):\n",
    "        super(MLP, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        if classification == \"binary\":\n",
    "            self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        else:\n",
    "            # For multi-classification\n",
    "            self.fc3 = nn.Linear(hidden_2, 4)\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-layer Perceptron using first 10 Word2Vec features as input features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class MLP_vec(nn.Module):\n",
    "    def __init__(self, classification=\"binary\", vocab_size=300):\n",
    "        super(MLP_vec, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        if classification == \"binary\":\n",
    "            self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        else:\n",
    "            # For multi-classification\n",
    "            self.fc3 = nn.Linear(hidden_2, 4)\n",
    "        self.prod = 10\n",
    "        self.fc1 = nn.Linear(vocab_size * self.prod, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1] * x.shape[2])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Loaders for Train Data and Test Data, which supplies reviews one by one from the batches to the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class trainData(Dataset):\n",
    "\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "class testData(Dataset):\n",
    "\n",
    "    def __init__(self, X_data, Y_data):\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.Y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "filename = \"./amazon_reviews_us_Kitchen_v1_00.tsv\"\n",
    "dt = DataTranformation(filename, True)\n",
    "\n",
    "train, test = dt.train_test_split()\n",
    "\n",
    "sentences = Sentence(train['review_body'])\n",
    "\n",
    "pretrained_model = api.load('word2vec-google-news-300')\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences, vector_size=300, min_count=10, window=11, seed=200)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Preproces: True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mean feature extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "vec_train = Vectorization(model=model, dataset=train)\n",
    "vec_test = Vectorization(model, test)\n",
    "\n",
    "X_train_model, Y_train_model = vec_train.feature_extraction()\n",
    "X_test_model, Y_test_model = vec_test.feature_extraction()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vectorization Completed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "MEAN MULTI-CLASS FEATURES EXTRACTION\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "vec_multi_train = Vectorization(model, train, classification=\"multi-class\")\n",
    "vec_multi_test = Vectorization(model, test, classification=\"multi-class\")\n",
    "\n",
    "X_train_multi, Y_train_multi = vec_multi_train.feature_extraction()\n",
    "X_test_multi, Y_test_multi = vec_multi_test.feature_extraction()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vectorization Completed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "TEN FEATURES IN A SINGLE ROW FEATURE EXTRACTION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "vec_mode_train = Vectorization(model, train, classification=\"binary\", mode=\"vec\")\n",
    "vec_mode_test = Vectorization(model, test, classification=\"binary\", mode=\"vec\")\n",
    "\n",
    "X_train_mode, Y_train_mode = vec_mode_train.feature_extraction()\n",
    "X_test_mode, Y_test_mode = vec_mode_test.feature_extraction()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vectorization Completed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "TEN FEATURES IN A SINGLE ROW MULTI-CLASS FEATURES EXTRACTION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "vec_mode_train_multi = Vectorization(model, train, classification=\"multi-class\", mode=\"vec\")\n",
    "vec_mode_test_multi = Vectorization(model, test, classification=\"multi-class\", mode=\"vec\")\n",
    "\n",
    "X_train_mode_multi, Y_train_mode_multi = vec_mode_train_multi.feature_extraction()\n",
    "X_test_mode_multi, Y_test_mode_multi = vec_mode_test_multi.feature_extraction()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vectorization Completed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "PRETRAINED MODEL FEATURES EXTRACTION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "vec2_train = Vectorization(model=pretrained_model, dataset=train, model_type=\"pretrained\")\n",
    "vec2_test = Vectorization(model=pretrained_model, dataset=test, model_type=\"pretrained\")\n",
    "\n",
    "X_train_pre, Y_train_pre = vec2_train.feature_extraction()\n",
    "X_test_pre, Y_test_pre = vec2_test.feature_extraction()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vectorization Completed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "PRETRAINED MODEL MULTI-CLASS FEATURES EXTRACTION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "vec2_multi_train = Vectorization(model=pretrained_model, dataset=train, classification=\"multi-class\", model_type=\"pretrained\")\n",
    "vec2_multi_test = Vectorization(model=pretrained_model, dataset=test, classification=\"multi-class\", model_type=\"pretrained\")\n",
    "\n",
    "X_train_multi_pre, Y_train_multi_pre = vec2_multi_train.feature_extraction()\n",
    "X_test_multi_pre, Y_test_multi_pre = vec2_multi_test.feature_extraction()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vectorization Completed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "PRETRAINED MODE VEC BINARY FEATURES EXTRACTION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "vec_mode_train_pre = Vectorization(model=pretrained_model, dataset=train, model_type=\"pretrained\", mode=\"vec\")\n",
    "vec_mode_test_pre = Vectorization(model=pretrained_model, dataset=test, model_type=\"pretrained\", mode=\"vec\")\n",
    "\n",
    "X_train_mode_pre, Y_train_mode_pre = vec_mode_train_pre.feature_extraction()\n",
    "X_test_mode_pre, Y_test_mode_pre = vec_mode_test_pre.feature_extraction()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vectorization Completed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "PRETRAINED MDOE VEC MULTI-CLASS FEATURES EXTRACTION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "vec_mode_train_multi_pre = Vectorization(model=pretrained_model, dataset=train, classification=\"multi-class\", model_type=\"pretrained\", mode=\"vec\")\n",
    "vec_mode_test_multi_pre = Vectorization(model=pretrained_model, dataset=test, classification=\"multi-class\", model_type=\"pretrained\", mode=\"vec\")\n",
    "\n",
    "X_train_mode_multi_pre, Y_train_mode_multi_pre = vec_mode_train_multi_pre.feature_extraction()\n",
    "X_test_mode_multi_pre, Y_test_mode_multi_pre = vec_mode_test_multi_pre.feature_extraction()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vectorization Completed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TRAINING FUNCTION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def training(model, epoch, dataset_x, dataset_y, name=\"model\"):\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    print(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(mlp_model.parameters(), lr=0.01)\n",
    "\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    training_data = trainData(torch.FloatTensor(\n",
    "        dataset_x), torch.LongTensor(dataset_y))\n",
    "    train_loader = DataLoader(\n",
    "        dataset=training_data, batch_size=16, shuffle=True)\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "\n",
    "        train_loss = 0.0\n",
    "\n",
    "        mlp_model.train()\n",
    "        for input_data, label in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = mlp_model(input_data.to(device))\n",
    "            # y_batch.unsqueeze(1) (label.unsqueeze(1)).to(device)\n",
    "            loss = criterion(output, label.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * input_data.size(1)\n",
    "\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    # print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "    torch.save(mlp_model.state_dict(), name + str(epoch + 1) + '.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TEST FUNCTION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def testing(model, epoch, dataset_x, dataset_y, name=\"model\"):\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    testing_data = testData(torch.FloatTensor(\n",
    "        dataset_x), torch.LongTensor(dataset_y))\n",
    "    test_loader = DataLoader(dataset=testing_data, batch_size=16)\n",
    "    tmp = 0\n",
    "    for i in range(1, epoch+1):\n",
    "        model.load_state_dict(torch.load(name + str(i) + '.pt'))\n",
    "        model = model.to(device)\n",
    "\n",
    "        predictions, actual = list(), list()\n",
    "        for test_data, test_label in test_loader:\n",
    "\n",
    "            pred = mlp_model(test_data.to(device))\n",
    "            pred = pred.detach().numpy()\n",
    "            pred = argmax(pred, axis=1)\n",
    "            target = test_label.numpy()\n",
    "            target = target.reshape((len(target), 1))\n",
    "            pred = pred.reshape((len(pred)), 1)\n",
    "            pred = pred.round()\n",
    "            predictions.append(pred)\n",
    "            actual.append(target)\n",
    "\n",
    "        predictions, actual = vstack(predictions), vstack(actual)\n",
    "        acc = accuracy_score(actual, predictions)\n",
    "        if acc > tmp:\n",
    "            tmp = acc\n",
    "    print('Accuracy: %.3f' % tmp)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "device = torch.device('cuda')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "\"\"\" BINARY-MEAN MLP \"\"\"\n",
    "\n",
    "mlp_model = MLP()  # binary classification\n",
    "training(mlp_model, 10, X_train_model, Y_train_model, name=\"mlp_model\")\n",
    "testing(mlp_model, 10, X_test_model, Y_test_model, name=\"mlp_model\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.788\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "\n",
    "\"\"\" MULTI-CLASS MEAN MLP \"\"\"\n",
    "\n",
    "mlp_model = MLP(classification=\"multi-class\")\n",
    "training(mlp_model, 10, X_train_multi, Y_train_multi, name=\"mlp_model_multi\")\n",
    "testing(mlp_model, 10, X_test_multi, Y_test_multi, name=\"mlp_model_multi\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.629\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "\n",
    "\"\"\" BINARY-VEC MLP \"\"\"\n",
    "\n",
    "mlp_model = MLP_vec()\n",
    "training(mlp_model, 10, X_train_mode, Y_train_mode, name=\"mlp_mode_vec\")\n",
    "testing(mlp_model, 10, X_test_mode, Y_test_mode, name=\"mlp_mode_vec\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.716\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "\n",
    "\"\"\" MULTI-VEC MLP \"\"\"\n",
    "\n",
    "mlp_model = MLP_vec(classification=\"multi-class\")\n",
    "training(mlp_model, 10, X_train_mode_multi, Y_train_mode_multi, name=\"mlp_mode_vec_multi\")\n",
    "testing(mlp_model, 10, X_test_mode_multi,\n",
    "        Y_test_mode_multi, name=\"mlp_mode_vec_multi\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.568\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "\n",
    "\"\"\" BINARY-MEAN PRETRAINED MLP \"\"\"\n",
    "mlp_model = MLP()\n",
    "training(mlp_model, 10, X_train_pre, Y_train_pre, name=\"mlp_model_pre\")\n",
    "testing(mlp_model, 10, X_test_pre, Y_test_pre, name=\"mlp_model_pre\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.838\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "\"\"\" MULTI-CLASS MEAN PRETRAINED MLP \"\"\"\n",
    "mlp_model = MLP(classification=\"multi-class\")\n",
    "training(mlp_model, 10, X_train_multi_pre, Y_train_multi_pre, name=\"mlp_mode_multi_pre\")\n",
    "testing(mlp_model, 10, X_test_multi_pre, Y_test_multi_pre, name=\"mlp_mode_multi_pre\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.679\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "\"\"\" BINARY VEC PRETRAINED MLP \"\"\"\n",
    "\n",
    "mlp_model = MLP_vec()\n",
    "training(mlp_model, 10, X_train_mode_pre, Y_train_mode_pre, name=\"mlp_vec_pre\")\n",
    "testing(mlp_model, 10, X_test_mode_pre, Y_test_mode_pre, name=\"mlp_vec_pre\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.755\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "\"\"\" MULTI-CLASS VEC PRETRAINED MLP \"\"\"\n",
    "\n",
    "mlp_model = MLP_vec(classification=\"multi-class\")\n",
    "training(mlp_model, 10, X_train_mode_multi_pre, Y_train_mode_multi_pre, name=\"mlp_vec_multi_pre\")\n",
    "testing(mlp_model, 10, X_test_mode_multi_pre, Y_test_mode_multi_pre, name=\"mlp_vec_multi_pre\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.608\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Observation:\n",
    "\n",
    "MLP trained using self-trained Word2Vec feature vectors produces better accuracy compared to pre-trained ones."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9cb23f9fb748a30e8d8bc81b9adb518e023d0772aa87870f409f813d831654e"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('ml': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}